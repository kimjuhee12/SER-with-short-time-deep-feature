{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Conv1D layer -> LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from emotion_recognition import utils\n",
    "from emotion_recognition import features\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.preprocessing import sequence\n",
    "from keras import layers\n",
    "from keras.models import Sequential\n",
    "from keras import regularizers\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import LSTM , RNN\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(action='ignore') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 Data preparation\n",
    "    min_length = 34510\n",
    "    max_length = 22230\n",
    "    Setting parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 34000 # 특징 추출을 위한 윈도우 크기\n",
    "class_size = 4 #class 크기\n",
    "sr = 16000\n",
    "\n",
    "# 0 = neutral, 1 = anger, 2 = happiness, 3 = sadness\n",
    "emotion = [\"neutral\",\"anger\",\"happiness\",\"sadness\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파일 이름 읽어오기\n",
    "path = \"../data/\"\n",
    "text_filename = \"filelist_wav.txt\"\n",
    "\n",
    "filelist_wav = []\n",
    "emotionlist = []\n",
    "\n",
    "f = open(path+text_filename, 'r')\n",
    "\n",
    "while True:\n",
    "    line = f.readline()\n",
    "    if not line: break\n",
    "    \n",
    "    filename, label = line.split()\n",
    "    \n",
    "    filelist_wav.append(filename)\n",
    "    emotionlist.append(label)\n",
    "    \n",
    "f.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "Training_filename = []\n",
    "Training_emotionlist = []\n",
    "Test_filename = []\n",
    "Test_emotionlist = []\n",
    "\n",
    "for i in range(len(filelist_wav)):\n",
    "    if (filelist_wav[i].split(\"_\")[0][1:] == '1'):\n",
    "        Test_filename.append(filelist_wav[i])\n",
    "        Test_emotionlist.append(emotionlist[i])\n",
    "    elif (filelist_wav[i].split(\"_\")[0][1:] == '6'):\n",
    "        Test_filename.append(filelist_wav[i])\n",
    "        Test_emotionlist.append(emotionlist[i])\n",
    "    elif (filelist_wav[i].split(\"_\")[0][1:] == '7'):\n",
    "        Test_filename.append(filelist_wav[i])\n",
    "        Test_emotionlist.append(emotionlist[i])\n",
    "    elif (filelist_wav[i].split(\"_\")[0][1:] == '11'):\n",
    "        Test_filename.append(filelist_wav[i])\n",
    "        Test_emotionlist.append(emotionlist[i])\n",
    "    else:\n",
    "        Training_filename.append(filelist_wav[i])\n",
    "        Training_emotionlist.append(emotionlist[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(614,)\n",
      "(226,)\n"
     ]
    }
   ],
   "source": [
    "print (np.shape(Training_filename)) # 614개 학습시키고 \n",
    "print (np.shape(Test_filename))     # 226개 테스트하기 총 840개(파일이름에 1,6,7,11 있는 것)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34000\n"
     ]
    }
   ],
   "source": [
    "Training_vector = []\n",
    "Training_label = []\n",
    "\n",
    "for ix in range(len(Training_filename)):\n",
    "    # 파일 읽어오기\n",
    "    if Training_emotionlist[ix] == \"excitement\":\n",
    "        continue\n",
    "        \n",
    "    if Training_emotionlist[ix] == \"fear\":\n",
    "        continue\n",
    "    \n",
    "    #print (str(ix)+ \"\\t\" + Training_emotionlist[ix] + \"\\t\" + path+'wav/' + Training_filename[ix])\n",
    "    \n",
    "    y,sr = utils.loadwav(path+'wav/' + Training_filename[ix])\n",
    "    \n",
    "    temp = emotion.index(Training_emotionlist[ix])    \n",
    "    label = np.zeros(class_size)\n",
    "    label[temp] = 1\n",
    "    \n",
    "    #파일 전체 길이\n",
    "    length = len(y)\n",
    "    \n",
    "    for i in range(0, length-window_size, int(window_size/3)): \n",
    "        y_sub = y[i:i+window_size]\n",
    "            \n",
    "        Training_vector.append(y_sub) # [[96000개], ........... ,[] ]=>총 204개(전체 파일 자른 개수)\n",
    "        Training_label.append(label)  # [0,0,0,1]식으로 감정표현\n",
    "        \n",
    "print(len(Training_vector[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\tsadness\t../data/wav/s7_sadness_M_s1.wav\n",
      "2\thappiness\t../data/wav/s11_happiness_F_h20.wav\n",
      "3\tsadness\t../data/wav/s11_sadness_F_s1.wav\n",
      "4\tsadness\t../data/wav/s11_sadness_F_s9.wav\n",
      "9\tneutral\t../data/wav/s1_neutral_M_n20.wav\n",
      "10\tsadness\t../data/wav/s6_sadness_F_s18.wav\n",
      "11\tanger\t../data/wav/s1_anger_M_a4.wav\n",
      "13\thappiness\t../data/wav/s6_happiness_F_h14.wav\n",
      "14\thappiness\t../data/wav/s11_happiness_F_h15.wav\n",
      "16\tanger\t../data/wav/s6_anger_F_a12.wav\n",
      "18\tsadness\t../data/wav/s11_sadness_F_s17.wav\n",
      "19\thappiness\t../data/wav/s6_happiness_F_h1.wav\n",
      "21\tsadness\t../data/wav/s6_sadness_F_s10.wav\n",
      "24\thappiness\t../data/wav/s6_happiness_F_h15.wav\n",
      "26\tneutral\t../data/wav/s6_neutral_F_n15.wav\n",
      "27\tsadness\t../data/wav/s7_sadness_M_s15.wav\n",
      "28\thappiness\t../data/wav/s6_happiness_F_h4.wav\n",
      "29\tsadness\t../data/wav/s7_sadness_M_s11.wav\n",
      "30\tsadness\t../data/wav/s6_sadness_F_s12.wav\n",
      "31\tanger\t../data/wav/s7_anger_M_a17.wav\n",
      "32\tneutral\t../data/wav/s1_neutral_M_n13.wav\n",
      "33\tanger\t../data/wav/s11_anger_F_a16.wav\n",
      "34\tsadness\t../data/wav/s7_sadness_M_s20.wav\n",
      "35\thappiness\t../data/wav/s11_happiness_F_h16.wav\n",
      "36\thappiness\t../data/wav/s6_happiness_F_h19.wav\n",
      "37\thappiness\t../data/wav/s6_happiness_F_h12.wav\n",
      "38\tanger\t../data/wav/s1_anger_M_a17.wav\n",
      "39\tsadness\t../data/wav/s6_sadness_F_s13.wav\n",
      "40\tanger\t../data/wav/s1_anger_M_a7.wav\n",
      "41\thappiness\t../data/wav/s11_happiness_F_h5.wav\n",
      "42\tanger\t../data/wav/s1_anger_M_a11.wav\n",
      "43\tanger\t../data/wav/s1_anger_M_a2.wav\n",
      "44\tneutral\t../data/wav/s1_neutral_M_n10.wav\n",
      "45\tanger\t../data/wav/s7_anger_M_a3.wav\n",
      "46\tanger\t../data/wav/s6_anger_F_a11.wav\n",
      "47\thappiness\t../data/wav/s6_happiness_F_h18.wav\n",
      "48\tsadness\t../data/wav/s11_sadness_F_s12.wav\n",
      "49\tneutral\t../data/wav/s7_neutral_M_n5.wav\n",
      "51\tneutral\t../data/wav/s6_neutral_F_n8.wav\n",
      "54\tanger\t../data/wav/s6_anger_F_a6.wav\n",
      "56\thappiness\t../data/wav/s7_happiness_M_h10.wav\n",
      "57\tanger\t../data/wav/s6_anger_F_a15.wav\n",
      "58\tanger\t../data/wav/s1_anger_M_a16.wav\n",
      "59\tanger\t../data/wav/s1_anger_M_a3.wav\n",
      "60\thappiness\t../data/wav/s7_happiness_M_h3.wav\n",
      "62\thappiness\t../data/wav/s11_happiness_F_h17.wav\n",
      "63\tsadness\t../data/wav/s6_sadness_F_s20.wav\n",
      "67\tneutral\t../data/wav/s6_neutral_F_n16.wav\n",
      "68\tanger\t../data/wav/s11_anger_F_a5.wav\n",
      "69\tneutral\t../data/wav/s6_neutral_F_n4.wav\n",
      "73\tanger\t../data/wav/s11_anger_F_a13.wav\n",
      "75\tanger\t../data/wav/s6_anger_F_a2.wav\n",
      "76\tsadness\t../data/wav/s7_sadness_M_s17.wav\n",
      "77\tanger\t../data/wav/s7_anger_M_a6.wav\n",
      "79\thappiness\t../data/wav/s7_happiness_M_h14.wav\n",
      "81\tsadness\t../data/wav/s7_sadness_M_s9.wav\n",
      "82\tneutral\t../data/wav/s1_neutral_M_n11.wav\n",
      "83\tanger\t../data/wav/s7_anger_M_a12.wav\n",
      "86\tneutral\t../data/wav/s6_neutral_F_n17.wav\n",
      "87\tneutral\t../data/wav/s11_neutral_F_n17.wav\n",
      "88\tanger\t../data/wav/s11_anger_F_a19.wav\n",
      "90\tanger\t../data/wav/s1_anger_M_a9.wav\n",
      "91\thappiness\t../data/wav/s6_happiness_F_h5.wav\n",
      "93\tsadness\t../data/wav/s6_sadness_F_s16.wav\n",
      "95\tanger\t../data/wav/s11_anger_F_a15.wav\n",
      "96\tsadness\t../data/wav/s7_sadness_M_s4.wav\n",
      "97\tanger\t../data/wav/s7_anger_M_a16.wav\n",
      "98\tneutral\t../data/wav/s7_neutral_M_n3.wav\n",
      "99\thappiness\t../data/wav/s6_happiness_F_h16.wav\n",
      "100\tanger\t../data/wav/s6_anger_F_a16.wav\n",
      "101\tanger\t../data/wav/s6_anger_F_a7.wav\n",
      "102\tsadness\t../data/wav/s11_sadness_F_s10.wav\n",
      "103\tanger\t../data/wav/s7_anger_M_a18.wav\n",
      "104\tneutral\t../data/wav/s11_neutral_F_n8.wav\n",
      "105\tneutral\t../data/wav/s11_neutral_F_n3.wav\n",
      "106\tsadness\t../data/wav/s6_sadness_F_s19.wav\n",
      "107\tsadness\t../data/wav/s6_sadness_F_s1.wav\n",
      "108\tanger\t../data/wav/s6_anger_F_a3.wav\n",
      "110\tanger\t../data/wav/s11_anger_F_a14.wav\n",
      "114\tsadness\t../data/wav/s7_sadness_M_s19.wav\n",
      "116\thappiness\t../data/wav/s11_happiness_F_h18.wav\n",
      "117\tanger\t../data/wav/s11_anger_F_a9.wav\n",
      "118\tsadness\t../data/wav/s11_sadness_F_s19.wav\n",
      "119\tanger\t../data/wav/s11_anger_F_a12.wav\n",
      "121\tanger\t../data/wav/s6_anger_F_a18.wav\n",
      "123\tanger\t../data/wav/s6_anger_F_a10.wav\n",
      "124\tsadness\t../data/wav/s11_sadness_F_s4.wav\n",
      "125\tsadness\t../data/wav/s6_sadness_F_s17.wav\n",
      "126\tneutral\t../data/wav/s6_neutral_F_n5.wav\n",
      "127\tsadness\t../data/wav/s7_sadness_M_s2.wav\n",
      "128\tanger\t../data/wav/s6_anger_F_a5.wav\n",
      "129\tneutral\t../data/wav/s11_neutral_F_n20.wav\n",
      "131\thappiness\t../data/wav/s6_happiness_F_h17.wav\n",
      "132\tanger\t../data/wav/s6_anger_F_a1.wav\n",
      "135\tsadness\t../data/wav/s11_sadness_F_s5.wav\n",
      "136\tsadness\t../data/wav/s6_sadness_F_s14.wav\n",
      "139\tanger\t../data/wav/s6_anger_F_a20.wav\n",
      "141\tanger\t../data/wav/s7_anger_M_a2.wav\n",
      "142\thappiness\t../data/wav/s6_happiness_F_h6.wav\n",
      "145\thappiness\t../data/wav/s11_happiness_F_h13.wav\n",
      "146\tanger\t../data/wav/s6_anger_F_a13.wav\n",
      "147\thappiness\t../data/wav/s1_happiness_M_h16.wav\n",
      "149\tanger\t../data/wav/s6_anger_F_a9.wav\n",
      "150\tsadness\t../data/wav/s7_sadness_M_s10.wav\n",
      "151\tneutral\t../data/wav/s7_neutral_M_n1.wav\n",
      "152\tanger\t../data/wav/s7_anger_M_a5.wav\n",
      "153\thappiness\t../data/wav/s11_happiness_F_h6.wav\n",
      "156\tneutral\t../data/wav/s11_neutral_F_n14.wav\n",
      "158\tneutral\t../data/wav/s7_neutral_M_n2.wav\n",
      "161\thappiness\t../data/wav/s7_happiness_M_h15.wav\n",
      "162\thappiness\t../data/wav/s11_happiness_F_h19.wav\n",
      "163\tsadness\t../data/wav/s11_sadness_F_s6.wav\n",
      "164\thappiness\t../data/wav/s11_happiness_F_h12.wav\n",
      "165\tanger\t../data/wav/s6_anger_F_a8.wav\n",
      "166\tanger\t../data/wav/s7_anger_M_a4.wav\n",
      "167\tsadness\t../data/wav/s7_sadness_M_s16.wav\n",
      "169\thappiness\t../data/wav/s6_happiness_F_h7.wav\n",
      "171\tanger\t../data/wav/s6_anger_F_a17.wav\n",
      "172\thappiness\t../data/wav/s6_happiness_F_h20.wav\n",
      "173\tneutral\t../data/wav/s1_neutral_M_n8.wav\n",
      "174\thappiness\t../data/wav/s11_happiness_F_h7.wav\n",
      "175\tsadness\t../data/wav/s6_sadness_F_s9.wav\n",
      "176\tanger\t../data/wav/s6_anger_F_a14.wav\n",
      "177\thappiness\t../data/wav/s6_happiness_F_h13.wav\n",
      "178\tsadness\t../data/wav/s6_sadness_F_s8.wav\n",
      "179\tsadness\t../data/wav/s6_sadness_F_s15.wav\n",
      "180\tanger\t../data/wav/s6_anger_F_a19.wav\n",
      "181\thappiness\t../data/wav/s6_happiness_F_h8.wav\n",
      "182\tsadness\t../data/wav/s1_sadness_M_s4.wav\n",
      "184\thappiness\t../data/wav/s6_happiness_F_h11.wav\n",
      "185\tsadness\t../data/wav/s11_sadness_F_s20.wav\n",
      "186\tneutral\t../data/wav/s7_neutral_M_n4.wav\n",
      "189\thappiness\t../data/wav/s7_happiness_M_h20.wav\n",
      "194\tneutral\t../data/wav/s1_neutral_M_n12.wav\n",
      "195\tanger\t../data/wav/s1_anger_M_a10.wav\n",
      "196\tneutral\t../data/wav/s1_neutral_M_n16.wav\n",
      "198\tneutral\t../data/wav/s11_neutral_F_n10.wav\n",
      "201\thappiness\t../data/wav/s7_happiness_M_h2.wav\n",
      "202\thappiness\t../data/wav/s11_happiness_F_h10.wav\n",
      "203\tanger\t../data/wav/s1_anger_M_a6.wav\n",
      "205\thappiness\t../data/wav/s6_happiness_F_h10.wav\n",
      "206\tanger\t../data/wav/s6_anger_F_a4.wav\n",
      "207\thappiness\t../data/wav/s11_happiness_F_h11.wav\n",
      "208\tneutral\t../data/wav/s6_neutral_F_n20.wav\n",
      "209\tneutral\t../data/wav/s11_neutral_F_n5.wav\n",
      "211\tneutral\t../data/wav/s11_neutral_F_n12.wav\n",
      "213\tanger\t../data/wav/s1_anger_M_a14.wav\n",
      "214\tneutral\t../data/wav/s11_neutral_F_n13.wav\n",
      "215\tsadness\t../data/wav/s11_sadness_F_s8.wav\n",
      "216\tanger\t../data/wav/s7_anger_M_a10.wav\n",
      "218\tneutral\t../data/wav/s7_neutral_M_n19.wav\n",
      "219\tneutral\t../data/wav/s7_neutral_M_n8.wav\n",
      "223\thappiness\t../data/wav/s7_happiness_M_h5.wav\n",
      "224\tanger\t../data/wav/s7_anger_M_a7.wav\n",
      "225\tneutral\t../data/wav/s11_neutral_F_n4.wav\n"
     ]
    }
   ],
   "source": [
    "Test_vector = []\n",
    "Test_label = []\n",
    "\n",
    "for ix in range(len(Test_filename)):\n",
    "    # 파일 읽어오기\n",
    "    if Test_emotionlist[ix] == \"excitement\":\n",
    "        continue\n",
    "        \n",
    "    if Test_emotionlist[ix] == \"fear\":\n",
    "        continue\n",
    "    \n",
    "    print (str(ix)+ \"\\t\" + Test_emotionlist[ix] + \"\\t\" + path+'wav/' + Test_filename[ix])\n",
    "    \n",
    "    y,sr = utils.loadwav(path+'wav/' + Test_filename[ix])\n",
    "    \n",
    "    temp = emotion.index(Test_emotionlist[ix])    \n",
    "    label = np.zeros(class_size)\n",
    "    label[temp] = 1\n",
    "    \n",
    "    #파일 전체 길이\n",
    "    length = len(y)\n",
    "    \n",
    "    for i in range(0, length-window_size, int(window_size/3)):\n",
    "        y_sub = y[i:i+window_size]\n",
    "            \n",
    "        Test_vector.append(y_sub)\n",
    "        Test_label.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2307, 34000) (1088, 34000)\n"
     ]
    }
   ],
   "source": [
    "print (np.shape(Training_vector), np.shape(Test_vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "for i in range(len(Training_vector)):\n",
    "    Training_vector[i] = Training_vector[i] * 2 / math.pow(2,16)\n",
    "    \n",
    "for i in range(len(Test_vector)):\n",
    "    Test_vector[i] = Test_vector[i] * 2 / math.pow(2,16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "Training_vector = np.expand_dims(Training_vector,axis=2)\n",
    "Test_vector = np.expand_dims(Test_vector,axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2307, 34000, 1) (1088, 34000, 1)\n"
     ]
    }
   ],
   "source": [
    "print (np.shape(Training_vector), np.shape(Test_vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(Training_vector)\n",
    "Y_train = np.array(Training_label)\n",
    "X_test = np.array(Test_vector)\n",
    "Y_test = np.array(Test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_index = np.arange(np.shape(X_train)[0])\n",
    "np.random.shuffle(random_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train[random_index]\n",
    "Y_train = Y_train[random_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2307, 34000, 1) (1088, 34000, 1)\n"
     ]
    }
   ],
   "source": [
    "print (np.shape(X_train), np.shape(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2307, 4) (1088, 4)\n"
     ]
    }
   ],
   "source": [
    "print (np.shape(Y_train), np.shape(Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_21 (Conv1D)           (None, 34000, 40)         3240      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling (None, 17000, 40)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_22 (Conv1D)           (None, 17000, 40)         6400040   \n",
      "_________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling (None, 850, 40)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_11 (Bidirectio (None, 256)               173056    \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 4)                 1028      \n",
      "=================================================================\n",
      "Total params: 6,577,364\n",
      "Trainable params: 6,577,364\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model_m = Sequential()\n",
    "model_m.add(layers.Conv1D(40, 80, activation='relu', padding='same', input_shape=(window_size, 1))) # Temporal convolution\n",
    "model_m.add(layers.MaxPooling1D(pool_size=2, strides =2))      # Pooling\n",
    "model_m.add(layers.Conv1D(40, 4000, padding = 'same',activation='relu')) # Temporal convolution\n",
    "model_m.add(layers.MaxPooling1D(pool_size=20, strides =20))     # MaxPooling\n",
    "model_m.add(Bidirectional(LSTM(128, input_shape=(850, 40) ))) #Recurrent layers\n",
    "model_m.add(layers.Dense(class_size, activation='softmax'))\n",
    "print(model_m.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/pmp/anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 2307 samples, validate on 1088 samples\n",
      "Epoch 1/400\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[1,17000,160000] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\n\t [[{{node training/Adam/gradients/conv1d_22/convolution_grad/Conv2DBackpropFilter}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-73483d6e478d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m                       \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m                       \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m                       verbose=1)\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3292\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[1,17000,160000] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\n\t [[{{node training/Adam/gradients/conv1d_22/convolution_grad/Conv2DBackpropFilter}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n"
     ]
    }
   ],
   "source": [
    "callbacks_list = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath='best_model.{epoch:02d}-{val_loss:.2f}.h5',\n",
    "        monitor='val_loss', save_best_only=True),\n",
    "    keras.callbacks.EarlyStopping(monitor='acc', patience=1)\n",
    "]\n",
    "\n",
    "model_m.compile(loss='categorical_crossentropy',\n",
    "                optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "BATCH_SIZE = 200\n",
    "EPOCHS = 400\n",
    "\n",
    "history = model_m.fit(X_train,\n",
    "                      Y_train,\n",
    "                      batch_size=BATCH_SIZE,\n",
    "                      epochs=EPOCHS,\n",
    "                      validation_data=(X_test,Y_test),\n",
    "                      verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history.history[\"acc\"]\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs= range(1,len(acc)+1)\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label = 'Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label = 'Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label = 'Training loss')\n",
    "plt.plot(epochs, val_loss,'b',label = 'Validation loss')\n",
    "plt.title('Training an validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc =model_m.evaluate(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (acc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
